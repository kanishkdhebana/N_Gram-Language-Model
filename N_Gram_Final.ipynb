{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "# Read the file into a Python variable\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlr2TWxenx2l",
        "outputId": "ce93444c-c77a-4fba-c4f3-00ec789f97d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-08 04:50:03--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.36M  21.7MB/s    in 0.2s    \n",
            "\n",
            "2025-03-08 04:50:04 (21.7 MB/s) - ‘input.txt’ saved [4573338/4573338]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QVAO98iLkox",
        "outputId": "47019786-737d-44ba-8ad8-79a6c65c96b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95RMZRJgV8Ui",
        "outputId": "b0c2e198-4d9e-43ab-e56c-73de1ba45131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  4573338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "ngrams = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
        "unique_ngrams = sorted(list(set(ngrams)))\n",
        "vocab_size = len(unique_ngrams)\n",
        "print(\"Vocabulary Size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J4E1VlIgBqT",
        "outputId": "574685d9-9adb-467c-c14a-ec8f96e829e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 15720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ng: i for i, ng in enumerate(unique_ngrams)}\n",
        "itos = {i: ng for i, ng in enumerate(unique_ngrams)}\n",
        "\n",
        "print(\"Sample of stoi:\", dict(list(stoi.items())[:10]))\n",
        "print(\"Sample of itos:\", dict(list(itos.items())[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUNwxdrzWJls",
        "outputId": "b901ea7d-10bc-4846-9c8b-8b3cc868318c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of stoi: {'\\n\\n\\n': 0, '\\n\\nA': 1, '\\n\\nB': 2, '\\n\\nC': 3, '\\n\\nD': 4, '\\n\\nE': 5, '\\n\\nF': 6, '\\n\\nG': 7, '\\n\\nH': 8, '\\n\\nI': 9}\n",
            "Sample of itos: {0: '\\n\\n\\n', 1: '\\n\\nA', 2: '\\n\\nB', 3: '\\n\\nC', 4: '\\n\\nD', 5: '\\n\\nE', 6: '\\n\\nF', 7: '\\n\\nG', 8: '\\n\\nH', 9: '\\n\\nI'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[s[i:i + n]] for i in range(len(s) - n + 1) if s[i:i + n] in stoi]\n",
        "\n",
        "def decode(l):\n",
        "    if not l:\n",
        "        return \"\"\n",
        "    decoded_string = itos[l[0]]\n",
        "    for i in range(1, len(l)):\n",
        "        decoded_string += itos[l[i]][-1]\n",
        "    return decoded_string\n",
        "\n",
        "# Testing the updated functions\n",
        "encoded = encode(\"who am i\")\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef35VXGrWz9H",
        "outputId": "0d27e0cb-55ac-48a7-e041-7a8003de0d51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [15066, 9428, 11783, 1054, 6580, 10896]\n",
            "Decoded: who am i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = int)\n",
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTQdcew8ZwIt",
        "outputId": "b69efdec-06fd-4523-dee2-6c5c6a09489b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3763,  9910, 13248, 13739, 13860,   770,  3262,  9962, 14101, 10004,\n",
              "        15680,  8237, 11323,  2386,   113,  3090,  8112,  8742, 12212, 12979,\n",
              "         7914,  1281, 15036,  7907,  1221, 12648, 13170, 11905,  7218,  8082,\n",
              "         8043,  7490,  1055,  6634, 11694, 15406,  1128,  8792, 14677, 13267,\n",
              "        14061,  9374,  8313, 12847,  1910,  1140,  9359,  8004,  6667, 12820,\n",
              "         1180, 11004,  7910,  1244, 13701, 12544,  7999,  6537, 10179,  2298,\n",
              "            1,    94,  2923, 10633, 10446,  2403,   376,  5693, 12544,  7999,\n",
              "         6535, 10151,  1921,  1244, 13701, 12544,  7999,  6537, 10179,  2298,\n",
              "            6,   187,  3763,  9910, 13248, 13739, 13860,   770,  3262,  9962,\n",
              "        14101, 10004, 15680,  8237, 11323,  2408,   431,  6213, 12280, 14309])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model dimensions\n",
        "n_embed = 256          # Size of embeddings (increased to handle large vocab)\n",
        "n_head = 8             # Number of attention heads\n",
        "n_layer = 6            # Number of transformer layers\n",
        "\n",
        "# Batch and context\n",
        "batch_size = 32        # Reduced to save memory\n",
        "block_size = 512       # Increased context length for better sequence understanding\n",
        "\n",
        "# Training parameters\n",
        "max_iters = 5000       # Training iterations\n",
        "eval_interval = 500    # Frequency of evaluation\n",
        "learning_rate = 5e-4   # Lowered for stable training with larger vocab\n",
        "eval_iters = 200       # Batches for evaluation\n",
        "\n",
        "# Regularization\n",
        "dropout = 0.2          # Dropout to prevent overfitting\n",
        "weight_decay = 1e-4    # Regularization to improve generalization\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "z8NEKZqAXEo8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "n = int(0.85 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "5vmHH7umZ52e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch_random(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    max_index = len(data) - block_size - 1\n",
        "    random_index = torch.randint(max_index, (batch_size,))\n",
        "    x = torch.stack([data[i: i + block_size] for i in random_index])\n",
        "    y = torch.stack([data[(i + 1): (i + 1) + block_size] for i in random_index])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch_random(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "x_batch, y_batch = get_batch_random('train')\n",
        "print('inputs:')\n",
        "print(x_batch.shape)\n",
        "print(x_batch)\n",
        "print('targets:')\n",
        "print(y_batch.shape)\n",
        "print(y_batch)\n"
      ],
      "metadata": {
        "id": "B8UdT3oWagT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9615fa-c5dd-4ec2-b706-9f8f8a9ab990"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([32, 512])\n",
            "tensor([[ 1207, 12199, 12827,  ..., 14061,  9370,  8231],\n",
            "        [13508,  6667, 12815,  ..., 13560,  8339, 13295],\n",
            "        [ 1255, 14061,  9350,  ..., 11413,  7892,  1055],\n",
            "        ...,\n",
            "        [14509,  9900, 12979,  ..., 11465,  8992,  8199],\n",
            "        [ 8323, 12981,  7931,  ..., 14686, 13443,  1850],\n",
            "        [ 4208,  4742,  3172,  ..., 11102, 12212, 12979]])\n",
            "targets:\n",
            "torch.Size([32, 512])\n",
            "tensor([[12199, 12827,  1255,  ...,  9370,  8231, 11274],\n",
            "        [ 6667, 12815,  1139,  ...,  8339, 13295, 14791],\n",
            "        [14061,  9350,  7892,  ...,  7892,  1055,  6627],\n",
            "        ...,\n",
            "        [ 9900, 12979,  7900,  ...,  8992,  8199, 10677],\n",
            "        [12981,  7931,  1777,  ..., 13443,  1850,   421],\n",
            "        [ 4742,  3172,  3475,  ..., 12212, 12979,  7897]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using bigram language model\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.self_attention_head = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.self_attention_head(self.ln1(x))\n",
        "        x = x + self.fforward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) #(Batch, Time, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets) # quality of logits based on targets\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = NgramLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "print(next(model.parameters()).device)  # Should print \"cuda:0\" if using GPU\n",
        "\n",
        "\n",
        "logits, loss = model(x_batch, y_batch)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype = torch.long).to(device)\n",
        "print(decode(model.generate(idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrahGtfZeDzc",
        "outputId": "935a8aa2-349d-4620-a86c-cfc74a692cd9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([16384, 15720])\n",
            "tensor(9.8266, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "\n",
            "rlawglpcRwic\n",
            "faekjbm orHap;eofz:h \n",
            " g?;soi. sm;slctQfur?Rew]PScpfo bOAgl-Lio,Dupsa.ed,l\n",
            "cHo?ure-uikJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "Xrc8GyfxjFXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for iter in range(max_iters):\n",
        "    if (iter % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    x_batch, y_batch = get_batch_random('train')\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "jCv_k4mqoWfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c01891d-bb8b-48e9-86e6-b7c5673b917b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 9.827683448791504, val loss: 9.8255\n",
            "step: 500, train loss: 1.6809524297714233, val loss: 1.9484\n",
            "step: 1000, train loss: 1.5908868312835693, val loss: 1.8834\n",
            "step: 1500, train loss: 1.4464364051818848, val loss: 1.7728\n",
            "step: 2000, train loss: 1.3373647928237915, val loss: 1.7080\n",
            "step: 2500, train loss: 1.2660856246948242, val loss: 1.6683\n",
            "step: 3000, train loss: 1.2172232866287231, val loss: 1.6565\n",
            "step: 3500, train loss: 1.1778159141540527, val loss: 1.6443\n",
            "step: 4000, train loss: 1.1415619850158691, val loss: 1.6385\n",
            "step: 4500, train loss: 1.112372636795044, val loss: 1.6510\n",
            "1.1112065315246582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'ngram_language_model.pth')\n"
      ],
      "metadata": {
        "id": "vXRaCmmRS4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NgramLanguageModel(vocab_size)  # Initialize model architecture\n",
        "model.load_state_dict(torch.load('ngram_language_model.pth'))  # Load saved parameters\n",
        "model = model.to(device)  # Move to GPU if available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_XJtJhCVt7t",
        "outputId": "82d9491e-89db-44ff-cd3f-18824cc2595f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-578b3135314c>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('ngram_language_model.pth'))  # Load saved parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
        "print(decode(model.generate(context, max_new_tokens = 400)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "VT9uiB-uVugJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762ea718-c5c0-4e43-fbe9-b8b116186a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "CASSANIUS:\n",
            "O, good do we indeed we will be in you pleased.\n",
            "\n",
            "CASCA:\n",
            "No, very hallow'd he he be window\n",
            "You are oft unsultitude and amiCAN:\n",
            "The highers of that have her yet not so appear\n",
            "A good commonweign e and the duke.\n",
            "\n",
            "CASSIUS:\n",
            "E'eed gracious privilly sea in the sharincle\n",
            "With and certainies thanks, it complexion.\n",
            "\n",
            "BRUTUS:\n",
            "'Tis Hermia, hear to whisper;\n",
            "And have her in Egypt to vice; and, asses th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            context_cond = context[:, -block_size:]\n",
        "            logits, _ = model(context_cond)\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "            top_k_probs /= top_k_probs.sum()  # Normalize\n",
        "            idx_next = torch.multinomial(top_k_probs, num_samples=1)\n",
        "            idx_next = top_k_indices.gather(1, idx_next)\n",
        "\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "    return context\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(generate_text(model, context, max_new_tokens=800, temperature=0.8, top_k=50)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "sBG-p9aAkl9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba539816-fc16-44f0-9963-b7466656e073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "VALENTINE:\n",
            "No, that I do perceive the world.\n",
            "\n",
            "PROTEUS:\n",
            "Thou shalt be caloud; it makes amazed; and therefore was,\n",
            "That should be merit.\n",
            "\n",
            "Sevention, by my life; I am the clerk,\n",
            "Bloody distrainched by the fool-book to me:\n",
            "There be lord to the Trojan shrill-shonest bed\n",
            "to be more more madest in the next heaven;\n",
            "When I am sure now when thou art the reasons therefell\n",
            "I call them to their father council, the shade at\n",
            "another breather.\n",
            "\n",
            "SILVIUS:\n",
            "What says the what this shoes this grace?\n",
            "\n",
            "SIMPLE:\n",
            "She have been since she should play far attired with him\n",
            "bunornet she hath something: if he\n",
            "she shall convenient can pluck the worst\n",
            "child-chide of the sight of her her mind.\n",
            "\n",
            "SIMONIDES:\n",
            "I will make a day ring, and throw my patient.\n",
            "\n",
            "SPEED:\n",
            "Who even needs will, make thee words in charge:\n",
            "I have now in the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.2):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Safeguard for block size\n",
        "            context_cond = context[:, -min(block_size, context.size(1)):]\n",
        "\n",
        "            # Check if context is empty\n",
        "            if context_cond.size(1) == 0:\n",
        "                raise ValueError(\"Context is empty!\")\n",
        "\n",
        "            logits, _ = model(context_cond)\n",
        "\n",
        "            # Check if logits are empty\n",
        "            if logits.size(1) == 0:\n",
        "                raise ValueError(\"Logits are empty!\")\n",
        "\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "\n",
        "            # Repetition penalty\n",
        "            for token_id in set(context.view(-1).tolist()):\n",
        "                logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k > 0:\n",
        "                top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "                probs = torch.zeros_like(probs).scatter(1, top_k_indices, top_k_probs)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "            for i in range(probs.size(0)):\n",
        "                probs[i, sorted_indices[i][sorted_indices_to_remove[i]]] = 0\n",
        "\n",
        "            probs /= probs.sum()  # Normalize after filtering\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "\n",
        "            # Early stopping if the model keeps repeating\n",
        "            if idx_next.item() == context[:, -2].item():\n",
        "                break\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"You are all resolved rather to die than to famish?\"\n",
        "encoded_prompt = encode(prompt)  # Make sure this function works for tri-grams\n",
        "context = torch.tensor(encoded_prompt, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "generated_tokens = generate_text(model, context, max_new_tokens=800, temperature=0.8, top_k=50, top_p=0.9, repetition_penalty=1.2)\n",
        "print(decode(generated_tokens[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezpn0uHdUT5",
        "outputId": "84260461-c219-4ed1-a44e-d205621f639e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "FALSTAFF:\n",
            "Mistress Ford's service of it.\n",
            "\n",
            "PRINCE HENRY:\n",
            "Why shall, sir, what we mend this hour is, thy\n",
            "king on and\n",
            "three for fear: he will be hardly out of your\n",
            "conscience. I am at\n",
            "wifest till a fire, or, as I humbly tender.\n",
            "The knave, good man, yet has much by an\n",
            "heavy counteries those unclay from him old.\n",
            "Go in carry it after above God's chamber,\n",
            "when he's death.\n",
            "\n",
            "POINS:\n",
            "I pray, I come not from you, if I\n",
            "may; it blastink thee bravely.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "How my mind, most, howlines,\n",
            "smaches: O, let me speak off him, in it\n",
            "his worth had wash'd against\n",
            "To try, better his business,--as, fully-rook, thus:\n",
            "Being near something: advised!\n",
            "\n",
            "SHYLOCK:\n",
            "'Twas garmed, Othello,\n",
            "Some little cup inch, earney,\n",
            "That, flier: take your mouth,\n",
            "As imperfection.\n",
            "Conduct you: stand, certain; and both\n",
            "talk away: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "KsfVf6uTdkvM",
        "outputId": "4cd61328-0407-4be8-b437-084814f8cc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index -1 is out of bounds for dimension 1 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-45e10dfc3f77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"In a distant future, where artificial intelligence governs the stars,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-4748fec4230a>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, context, max_new_tokens, temperature, top_k, top_p, repetition_penalty)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mcontext_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m  \u001b[0;31m# Adjust randomness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Repetition penalty: penalize previously used tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SELF ATTENTION***"
      ],
      "metadata": {
        "id": "DzMtfLtPbf-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "x_bag_of_words = torch.zeros((B, T, C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        x_prev = x[b, :t + 1] # t, C\n",
        "        x_bag_of_words[b, t] = torch.mean(x_prev, 0)"
      ],
      "metadata": {
        "id": "PnI3jGKNWaTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2 (optimize above one)\n",
        "wei = torch.tril(torch.ones(T, T)) # crete bottom triangle matrix(can average the ones which are 1, else not)\n",
        "wei = wei / wei.sum(1, keepdim = True)\n",
        "x_bag_of_words2 = wei @ x # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "x_bag_of_words2"
      ],
      "metadata": {
        "id": "rkF59G_Vh-12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(x_bag_of_words, x_bag_of_words2) # if True -> both same"
      ],
      "metadata": {
        "id": "QSNof9F-iTGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3 (Softmax)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))"
      ],
      "metadata": {
        "id": "zdZXxv7TiUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SELF ATTENTION\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Single head of self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "#out = wei @ x\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape\n"
      ],
      "metadata": {
        "id": "dXFtm0iaaEHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CODE**"
      ],
      "metadata": {
        "id": "G9-fDUINMztP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STdbFUElPHVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}