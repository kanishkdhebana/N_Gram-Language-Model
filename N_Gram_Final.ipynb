{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "# Read the file into a Python variable\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlr2TWxenx2l",
        "outputId": "76cc89ec-f2e9-4491-e713-cb45d72c2c56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 12:04:26--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.36M  3.56MB/s    in 1.2s    \n",
            "\n",
            "2025-03-10 12:04:28 (3.56 MB/s) - ‘input.txt’ saved [4573338/4573338]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QVAO98iLkox",
        "outputId": "ba0af7c1-8b1b-4e4b-8426-bfa1a5386e1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95RMZRJgV8Ui",
        "outputId": "79aa7b6b-36de-49b8-94f1-78b531131c94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  4573338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "\n",
        "ngrams = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
        "unique_ngrams = sorted(list(set(ngrams)))\n",
        "vocab_size = len(unique_ngrams)\n",
        "print(\"Vocabulary Size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J4E1VlIgBqT",
        "outputId": "0aca64e2-afb0-4e50-f591-4f35cf904ef1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 15720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ng: i for i, ng in enumerate(unique_ngrams)}\n",
        "itos = {i: ng for i, ng in enumerate(unique_ngrams)}\n",
        "\n",
        "print(\"Sample of stoi:\", dict(list(stoi.items())[:10]))\n",
        "print(\"Sample of itos:\", dict(list(itos.items())[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUNwxdrzWJls",
        "outputId": "595fd642-0503-4955-e12b-32d405a60213"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of stoi: {'\\n\\n\\n': 0, '\\n\\nA': 1, '\\n\\nB': 2, '\\n\\nC': 3, '\\n\\nD': 4, '\\n\\nE': 5, '\\n\\nF': 6, '\\n\\nG': 7, '\\n\\nH': 8, '\\n\\nI': 9}\n",
            "Sample of itos: {0: '\\n\\n\\n', 1: '\\n\\nA', 2: '\\n\\nB', 3: '\\n\\nC', 4: '\\n\\nD', 5: '\\n\\nE', 6: '\\n\\nF', 7: '\\n\\nG', 8: '\\n\\nH', 9: '\\n\\nI'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[s[i:i + n]] for i in range(len(s) - n + 1) if s[i:i + n] in stoi]\n",
        "\n",
        "def decode(l):\n",
        "    if not l:\n",
        "        return \"\"\n",
        "\n",
        "    decoded_string = itos[l[0]]\n",
        "    for i in range(1, len(l)):\n",
        "        decoded_string += itos[l[i]][-1]\n",
        "    return decoded_string\n",
        "\n",
        "# Testing the updated functions\n",
        "encoded = encode(\"who am i\")\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef35VXGrWz9H",
        "outputId": "a6ec11b4-b0be-494e-a1d1-db796a3e2c3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [15066, 9428, 11783, 1054, 6580, 10896]\n",
            "Decoded: who am i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = int)\n",
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTQdcew8ZwIt",
        "outputId": "9b765e90-64a5-489a-838f-05af6c73e7c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3763,  9910, 13248, 13739, 13860,   770,  3262,  9962, 14101, 10004,\n",
              "        15680,  8237, 11323,  2386,   113,  3090,  8112,  8742, 12212, 12979,\n",
              "         7914,  1281, 15036,  7907,  1221, 12648, 13170, 11905,  7218,  8082,\n",
              "         8043,  7490,  1055,  6634, 11694, 15406,  1128,  8792, 14677, 13267,\n",
              "        14061,  9374,  8313, 12847,  1910,  1140,  9359,  8004,  6667, 12820,\n",
              "         1180, 11004,  7910,  1244, 13701, 12544,  7999,  6537, 10179,  2298,\n",
              "            1,    94,  2923, 10633, 10446,  2403,   376,  5693, 12544,  7999,\n",
              "         6535, 10151,  1921,  1244, 13701, 12544,  7999,  6537, 10179,  2298,\n",
              "            6,   187,  3763,  9910, 13248, 13739, 13860,   770,  3262,  9962,\n",
              "        14101, 10004, 15680,  8237, 11323,  2408,   431,  6213, 12280, 14309])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "n_embed = 256          # Size of embeddings\n",
        "n_head = 8             # Number of attention heads\n",
        "n_layer = 6            # Number of transformer layers\n",
        "batch_size = 32        # Batch size\n",
        "block_size = 512       # Context length\n",
        "max_iters = 5000       # Training iterations\n",
        "eval_interval = 500    # Evaluation interval\n",
        "learning_rate = 5e-4   # Learning rate\n",
        "eval_iters = 200       # Evaluation steps for validation\n",
        "dropout = 0.2          # Dropout probability\n",
        "weight_decay = 1e-4    # Weight decay for regularization\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n"
      ],
      "metadata": {
        "id": "z8NEKZqAXEo8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "n = int(0.85 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "5vmHH7umZ52e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch_random(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    max_index = len(data) - block_size - 1\n",
        "    random_index = torch.randint(max_index, (batch_size,))\n",
        "    x = torch.stack([data[i: i + block_size] for i in random_index])\n",
        "    y = torch.stack([data[(i + 1): (i + 1) + block_size] for i in random_index])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch_random(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "x_batch, y_batch = get_batch_random('train')\n",
        "print('inputs:')\n",
        "print(x_batch.shape)\n",
        "print(x_batch)\n",
        "print('targets:')\n",
        "print(y_batch.shape)\n",
        "print(y_batch)\n"
      ],
      "metadata": {
        "id": "B8UdT3oWagT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c54a0d-eb61-4c85-baa9-bd3b5b3a6088"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([32, 512])\n",
            "tensor([[ 1207, 12199, 12827,  ..., 14061,  9370,  8231],\n",
            "        [13508,  6667, 12815,  ..., 13560,  8339, 13295],\n",
            "        [ 1255, 14061,  9350,  ..., 11413,  7892,  1055],\n",
            "        ...,\n",
            "        [14509,  9900, 12979,  ..., 11465,  8992,  8199],\n",
            "        [ 8323, 12981,  7931,  ..., 14686, 13443,  1850],\n",
            "        [ 4208,  4742,  3172,  ..., 11102, 12212, 12979]])\n",
            "targets:\n",
            "torch.Size([32, 512])\n",
            "tensor([[12199, 12827,  1255,  ...,  9370,  8231, 11274],\n",
            "        [ 6667, 12815,  1139,  ...,  8339, 13295, 14791],\n",
            "        [14061,  9350,  7892,  ...,  7892,  1055,  6627],\n",
            "        ...,\n",
            "        [ 9900, 12979,  7900,  ...,  8992,  8199, 10677],\n",
            "        [12981,  7931,  1777,  ..., 13443,  1850,   421],\n",
            "        [ 4742,  3172,  3475,  ..., 12212, 12979,  7897]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using n-gram language model\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.self_attention_head = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.self_attention_head(self.ln1(x))\n",
        "        x = x + self.fforward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) #(Batch, Time, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets) # quality of logits based on targets\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = NgramLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "print(next(model.parameters()).device)  # Should print \"cuda:0\" if using GPU\n",
        "\n",
        "\n",
        "logits, loss = model(x_batch, y_batch)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype = torch.long).to(device)\n",
        "print(decode(model.generate(idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrahGtfZeDzc",
        "outputId": "79819446-b5bc-499e-8543-18e2f77973d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([16384, 15720])\n",
            "tensor(9.8266, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "\n",
            "rlawglpcRwic\n",
            "faekjbm orHap;eofz:h \n",
            " g?;soi. sm;slctQfur?Rew]PScpfo bOAgl-Lio,Dupsa.ed,l\n",
            "cHo?ure-uikJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2vazQHn8Wa8",
        "outputId": "f373e0f0-a7f4-4e82-a6f5-c7a7358a5a2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 12929896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(f\"The model has {num_params} trainable parameters.\")\n",
        "\n",
        "def count_parameters_by_layer(model):\n",
        "    total_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_params = param.numel()\n",
        "            print(f\"Layer: {name}, Parameters: {num_params}\")\n",
        "            total_params += num_params\n",
        "    print(f\"Total Trainable Parameters: {total_params}\")\n",
        "\n",
        "count_parameters_by_layer(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp183kUO9GG5",
        "outputId": "dabdd448-01d3-44ee-c5b9-d4c7189c1102"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 12929896 trainable parameters.\n",
            "Layer: token_embedding_table.weight, Parameters: 4024320\n",
            "Layer: position_embedding_table.weight, Parameters: 131072\n",
            "Layer: blocks.0.self_attention_head.heads.0.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.0.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.0.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.1.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.1.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.1.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.2.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.2.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.2.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.3.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.3.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.3.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.4.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.4.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.4.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.5.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.5.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.5.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.6.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.6.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.6.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.7.key.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.7.query.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.heads.7.value.weight, Parameters: 8192\n",
            "Layer: blocks.0.self_attention_head.projection.weight, Parameters: 65536\n",
            "Layer: blocks.0.self_attention_head.projection.bias, Parameters: 256\n",
            "Layer: blocks.0.fforward.net.0.weight, Parameters: 262144\n",
            "Layer: blocks.0.fforward.net.0.bias, Parameters: 1024\n",
            "Layer: blocks.0.fforward.net.2.weight, Parameters: 262144\n",
            "Layer: blocks.0.fforward.net.2.bias, Parameters: 256\n",
            "Layer: blocks.0.ln1.weight, Parameters: 256\n",
            "Layer: blocks.0.ln1.bias, Parameters: 256\n",
            "Layer: blocks.0.ln2.weight, Parameters: 256\n",
            "Layer: blocks.0.ln2.bias, Parameters: 256\n",
            "Layer: blocks.1.self_attention_head.heads.0.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.0.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.0.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.1.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.1.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.1.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.2.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.2.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.2.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.3.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.3.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.3.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.4.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.4.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.4.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.5.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.5.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.5.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.6.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.6.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.6.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.7.key.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.7.query.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.heads.7.value.weight, Parameters: 8192\n",
            "Layer: blocks.1.self_attention_head.projection.weight, Parameters: 65536\n",
            "Layer: blocks.1.self_attention_head.projection.bias, Parameters: 256\n",
            "Layer: blocks.1.fforward.net.0.weight, Parameters: 262144\n",
            "Layer: blocks.1.fforward.net.0.bias, Parameters: 1024\n",
            "Layer: blocks.1.fforward.net.2.weight, Parameters: 262144\n",
            "Layer: blocks.1.fforward.net.2.bias, Parameters: 256\n",
            "Layer: blocks.1.ln1.weight, Parameters: 256\n",
            "Layer: blocks.1.ln1.bias, Parameters: 256\n",
            "Layer: blocks.1.ln2.weight, Parameters: 256\n",
            "Layer: blocks.1.ln2.bias, Parameters: 256\n",
            "Layer: blocks.2.self_attention_head.heads.0.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.0.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.0.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.1.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.1.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.1.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.2.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.2.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.2.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.3.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.3.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.3.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.4.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.4.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.4.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.5.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.5.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.5.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.6.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.6.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.6.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.7.key.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.7.query.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.heads.7.value.weight, Parameters: 8192\n",
            "Layer: blocks.2.self_attention_head.projection.weight, Parameters: 65536\n",
            "Layer: blocks.2.self_attention_head.projection.bias, Parameters: 256\n",
            "Layer: blocks.2.fforward.net.0.weight, Parameters: 262144\n",
            "Layer: blocks.2.fforward.net.0.bias, Parameters: 1024\n",
            "Layer: blocks.2.fforward.net.2.weight, Parameters: 262144\n",
            "Layer: blocks.2.fforward.net.2.bias, Parameters: 256\n",
            "Layer: blocks.2.ln1.weight, Parameters: 256\n",
            "Layer: blocks.2.ln1.bias, Parameters: 256\n",
            "Layer: blocks.2.ln2.weight, Parameters: 256\n",
            "Layer: blocks.2.ln2.bias, Parameters: 256\n",
            "Layer: blocks.3.self_attention_head.heads.0.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.0.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.0.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.1.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.1.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.1.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.2.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.2.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.2.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.3.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.3.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.3.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.4.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.4.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.4.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.5.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.5.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.5.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.6.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.6.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.6.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.7.key.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.7.query.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.heads.7.value.weight, Parameters: 8192\n",
            "Layer: blocks.3.self_attention_head.projection.weight, Parameters: 65536\n",
            "Layer: blocks.3.self_attention_head.projection.bias, Parameters: 256\n",
            "Layer: blocks.3.fforward.net.0.weight, Parameters: 262144\n",
            "Layer: blocks.3.fforward.net.0.bias, Parameters: 1024\n",
            "Layer: blocks.3.fforward.net.2.weight, Parameters: 262144\n",
            "Layer: blocks.3.fforward.net.2.bias, Parameters: 256\n",
            "Layer: blocks.3.ln1.weight, Parameters: 256\n",
            "Layer: blocks.3.ln1.bias, Parameters: 256\n",
            "Layer: blocks.3.ln2.weight, Parameters: 256\n",
            "Layer: blocks.3.ln2.bias, Parameters: 256\n",
            "Layer: blocks.4.self_attention_head.heads.0.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.0.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.0.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.1.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.1.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.1.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.2.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.2.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.2.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.3.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.3.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.3.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.4.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.4.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.4.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.5.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.5.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.5.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.6.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.6.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.6.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.7.key.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.7.query.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.heads.7.value.weight, Parameters: 8192\n",
            "Layer: blocks.4.self_attention_head.projection.weight, Parameters: 65536\n",
            "Layer: blocks.4.self_attention_head.projection.bias, Parameters: 256\n",
            "Layer: blocks.4.fforward.net.0.weight, Parameters: 262144\n",
            "Layer: blocks.4.fforward.net.0.bias, Parameters: 1024\n",
            "Layer: blocks.4.fforward.net.2.weight, Parameters: 262144\n",
            "Layer: blocks.4.fforward.net.2.bias, Parameters: 256\n",
            "Layer: blocks.4.ln1.weight, Parameters: 256\n",
            "Layer: blocks.4.ln1.bias, Parameters: 256\n",
            "Layer: blocks.4.ln2.weight, Parameters: 256\n",
            "Layer: blocks.4.ln2.bias, Parameters: 256\n",
            "Layer: blocks.5.self_attention_head.heads.0.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.0.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.0.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.1.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.1.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.1.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.2.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.2.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.2.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.3.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.3.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.3.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.4.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.4.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.4.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.5.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.5.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.5.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.6.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.6.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.6.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.7.key.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.7.query.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.heads.7.value.weight, Parameters: 8192\n",
            "Layer: blocks.5.self_attention_head.projection.weight, Parameters: 65536\n",
            "Layer: blocks.5.self_attention_head.projection.bias, Parameters: 256\n",
            "Layer: blocks.5.fforward.net.0.weight, Parameters: 262144\n",
            "Layer: blocks.5.fforward.net.0.bias, Parameters: 1024\n",
            "Layer: blocks.5.fforward.net.2.weight, Parameters: 262144\n",
            "Layer: blocks.5.fforward.net.2.bias, Parameters: 256\n",
            "Layer: blocks.5.ln1.weight, Parameters: 256\n",
            "Layer: blocks.5.ln1.bias, Parameters: 256\n",
            "Layer: blocks.5.ln2.weight, Parameters: 256\n",
            "Layer: blocks.5.ln2.bias, Parameters: 256\n",
            "Layer: ln_f.weight, Parameters: 256\n",
            "Layer: ln_f.bias, Parameters: 256\n",
            "Layer: language_model_head.weight, Parameters: 4024320\n",
            "Layer: language_model_head.bias, Parameters: 15720\n",
            "Total Trainable Parameters: 12929896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "Xrc8GyfxjFXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for iter in range(max_iters):\n",
        "    if (iter % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    x_batch, y_batch = get_batch_random('train')\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "jCv_k4mqoWfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "248c2511-63d9-4365-88d7-5fc3a2eb191e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2474f110f46e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-a8d87fa0b504>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bee6dd3029dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bee6dd3029dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bee6dd3029dc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'ngram_language_model.pth')\n"
      ],
      "metadata": {
        "id": "vXRaCmmRS4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NgramLanguageModel(vocab_size)  # Initialize model architecture\n",
        "\n",
        "# If using CPU\n",
        "model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# If using GPU\n",
        "# model.load_state_dict(torch.load('ngram_language_model.pth'))  # Load saved parameters\n",
        "# model = model.to(device)  # Move to GPU if available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_XJtJhCVt7t",
        "outputId": "ed7b5a05-35d4-429b-e86f-faf6345647ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-21a37360d4e6>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
        "print(decode(model.generate(context, max_new_tokens = 400)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "VT9uiB-uVugJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b722ec-b293-4dfc-8109-1fd90930b7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "t him, his for a\n",
            "skilliing: he'll reconce again for my husband or an honesty move\n",
            "fair and corrupt, when his passions.\n",
            "\n",
            "DONhallenge is a man who meaning is very, an\n",
            "excelless it your plence.\n",
            "\n",
            "LEONATO:\n",
            "'I l said, I do tell you what you, Benedick?\n",
            "To-morrow at your voice. If I can, go afford\n",
            "I shall give the tongue. If I cannot conceive myself!\n",
            "it.\n",
            "\n",
            "DON PEDRO:\n",
            "And your company unwith, and dishones o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            context_cond = context[:, -block_size:]\n",
        "            logits, _ = model(context_cond)\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "            top_k_probs /= top_k_probs.sum()  # Normalize\n",
        "            idx_next = torch.multinomial(top_k_probs, num_samples=1)\n",
        "            idx_next = top_k_indices.gather(1, idx_next)\n",
        "\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "    return context\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(generate_text(model, context, max_new_tokens=800, temperature=1.2, top_k=50)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "sBG-p9aAkl9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb7d0ab-5eef-4ddb-8803-5ef0c65a7613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "CASSIO:\n",
            "If every gract it.\n",
            "\n",
            "LEONATO:\n",
            "I think\n",
            "Yd you wisdom an hours, sir, and now wicked\n",
            "dare my heart in dustine and begging.\n",
            "\n",
            "LEONATO:\n",
            "Your plauth.\n",
            "\n",
            "CASSAEDICK:\n",
            "A kinger? O, kill o' my sword mistress, I'll be hone.\n",
            "\n",
            "So:\n",
            "In he you are the gross.\n",
            "\n",
            "CLAUDIO:\n",
            "God! no, But lor tempo: good nest quitness, puts; and\n",
            "You would, give me awhile.\n",
            "At 'Where's your name, if you were to see it of?\n",
            "Or may call you must accustinted him:\n",
            "Come, let's some see: O, whitend, Escention\n",
            "It was a kind, and giving find it.\n",
            "How now, I kept, sir? Here's intent.\n",
            "Come, generatch ye's lodging, good again.\n",
            "Where yous Launce will you draw the woman Blay the\n",
            "That lies you asister'd chastised to his curiors.\n",
            "\n",
            "SEBASTIAN:\n",
            "Learnest us, my hand, sweet Ovide: O help to me\n",
            "The mind of the pearl accountaerles I wastess'd.\n",
            "But whe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, context, max_new_tokens, temperature = 0.7, top_k = 40, top_p = 0.9, repetition_penalty = 1.2):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Safeguard for block size\n",
        "            context_cond = context[:, -min(block_size, context.size(1)):]\n",
        "\n",
        "            # Check if context is empty\n",
        "            if context_cond.size(1) == 0:\n",
        "                raise ValueError(\"Context is empty!\")\n",
        "\n",
        "            logits, _ = model(context_cond)\n",
        "\n",
        "            # Check if logits are empty\n",
        "            if logits.size(1) == 0:\n",
        "                raise ValueError(\"Logits are empty!\")\n",
        "\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "\n",
        "            # Repetition penalty\n",
        "            for token_id in set(context.view(-1).tolist()):\n",
        "                logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k > 0:\n",
        "                top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "                probs = torch.zeros_like(probs).scatter(1, top_k_indices, top_k_probs)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "            for i in range(probs.size(0)):\n",
        "                probs[i, sorted_indices[i][sorted_indices_to_remove[i]]] = 0\n",
        "\n",
        "            probs /= probs.sum()  # Normalize after filtering\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "\n",
        "            # Early stopping if the model keeps repeating\n",
        "            if idx_next.item() == context[:, -2].item():\n",
        "                break\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"You are all resolved rather to die than to famish?\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor(encoded_prompt, dtype = torch.long, device = device).unsqueeze(0)\n",
        "\n",
        "generated_tokens = generate_text(model, context, max_new_tokens = 800, temperature = 0.5, top_k = 50, top_p = 0.8, repetition_penalty = 1.2)\n",
        "print(decode(generated_tokens[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "xezpn0uHdUT5",
        "outputId": "5e620910-a568-468d-c5a6-2996bc202151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Context is empty!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-677931165ff6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mgenerated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-677931165ff6>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, context, max_new_tokens, temperature, top_k, top_p, repetition_penalty)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# Check if context is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontext_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Context is empty!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Context is empty!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SELF ATTENTION***"
      ],
      "metadata": {
        "id": "DzMtfLtPbf-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "x_bag_of_words = torch.zeros((B, T, C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        x_prev = x[b, :t + 1] # t, C\n",
        "        x_bag_of_words[b, t] = torch.mean(x_prev, 0)"
      ],
      "metadata": {
        "id": "PnI3jGKNWaTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2 (optimize above one)\n",
        "wei = torch.tril(torch.ones(T, T)) # crete bottom triangle matrix(can average the ones which are 1, else not)\n",
        "wei = wei / wei.sum(1, keepdim = True)\n",
        "x_bag_of_words2 = wei @ x # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "x_bag_of_words2"
      ],
      "metadata": {
        "id": "rkF59G_Vh-12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(x_bag_of_words, x_bag_of_words2) # if True -> both same"
      ],
      "metadata": {
        "id": "QSNof9F-iTGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3 (Softmax)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))"
      ],
      "metadata": {
        "id": "zdZXxv7TiUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SELF ATTENTION\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Single head of self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "#out = wei @ x\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape\n"
      ],
      "metadata": {
        "id": "dXFtm0iaaEHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CODE**"
      ],
      "metadata": {
        "id": "G9-fDUINMztP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Download the Shakespeare dataset\n",
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "# Read the file into a Python variable\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "# Download the Shakespeare dataset\n",
        "n = 3\n",
        "ngrams = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
        "unique_ngrams = sorted(list(set(ngrams)))\n",
        "vocab_size = len(unique_ngrams)\n",
        "\n",
        "\n",
        "# Create mappings from n-grams to indices and vice versa\n",
        "stoi = {ng: i for i, ng in enumerate(unique_ngrams)}\n",
        "itos = {i: ng for i, ng in enumerate(unique_ngrams)}\n",
        "\n",
        "\n",
        "# Encode function: Convert string to a list of indices\n",
        "encode = lambda s: [stoi[s[i:i + n]] for i in range(len(s) - n + 1) if s[i:i + n] in stoi]\n",
        "\n",
        "\n",
        "# Decode function: Convert list of indices back to string\n",
        "def decode(l):\n",
        "    if not l:\n",
        "        return \"\"\n",
        "    decoded_string = itos[l[0]]\n",
        "    for i in range(1, len(l)):\n",
        "        decoded_string += itos[l[i]][-1]\n",
        "    return decoded_string\n",
        "\n",
        "\n",
        "# Encode the text into indices and convert to a tensor\n",
        "data = torch.tensor(encode(text), dtype = int)\n",
        "\n",
        "\n",
        "# Model hyperparameters\n",
        "n_embed = 256          # Size of embeddings\n",
        "n_head = 8             # Number of attention heads\n",
        "n_layer = 6            # Number of transformer layers\n",
        "batch_size = 32        # Batch size\n",
        "block_size = 512       # Context length\n",
        "max_iters = 5000       # Training iterations\n",
        "eval_interval = 500    # Evaluation interval\n",
        "learning_rate = 5e-4   # Learning rate\n",
        "eval_iters = 200       # Evaluation steps for validation\n",
        "dropout = 0.2          # Dropout probability\n",
        "weight_decay = 1e-4    # Weight decay for regularization\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
        "\n",
        "\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "n = int(0.85 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# Fix seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Function to get a random batch of data\n",
        "def get_batch_random(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    max_index = len(data) - block_size - 1\n",
        "    random_index = torch.randint(max_index, (batch_size,))\n",
        "    x = torch.stack([data[i: i + block_size] for i in random_index])\n",
        "    y = torch.stack([data[(i + 1): (i + 1) + block_size] for i in random_index])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Function to estimate loss without updating model\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch_random(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Define Transformer-based language model components\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Self-attention head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Self-attention head\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Feed-forward network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Transformer block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.self_attention_head = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.self_attention_head(self.ln1(x))\n",
        "        x = x + self.fforward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Language model class\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) #(Batch, Time, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets) # quality of logits based on targets\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "model = NgramLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    if (iter % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    x_batch, y_batch = get_batch_random('train')\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'ngram_language_model.pth')\n",
        "\n",
        "# Load model for inference\n",
        "model = NgramLanguageModel(vocab_size)  # Initialize model architecture\n",
        "\n",
        "# If using CPU\n",
        "model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# If using GPU\n",
        "# model.load_state_dict(torch.load('ngram_language_model.pth'))  # Load saved parameters\n",
        "#model = model.to(device)  # Move to GPU if available\n",
        "\n",
        "\n",
        "# Generate text based on a prompt\n",
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.2):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Safeguard for block size\n",
        "            context_cond = context[:, -min(block_size, context.size(1)):]\n",
        "\n",
        "            # Check if context is empty\n",
        "            if context_cond.size(1) == 0:\n",
        "                raise ValueError(\"Context is empty!\")\n",
        "\n",
        "            logits, _ = model(context_cond)\n",
        "\n",
        "            # Check if logits are empty\n",
        "            if logits.size(1) == 0:\n",
        "                raise ValueError(\"Logits are empty!\")\n",
        "\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "\n",
        "            # Repetition penalty\n",
        "            for token_id in set(context.view(-1).tolist()):\n",
        "                logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k > 0:\n",
        "                top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "                probs = torch.zeros_like(probs).scatter(1, top_k_indices, top_k_probs)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "            for i in range(probs.size(0)):\n",
        "                probs[i, sorted_indices[i][sorted_indices_to_remove[i]]] = 0\n",
        "\n",
        "            probs /= probs.sum()  # Normalize after filtering\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "\n",
        "            # Early stopping if the model keeps repeating\n",
        "            if idx_next.item() == context[:, -2].item():\n",
        "                break\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"You are all resolved rather to die than to famish?\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor(encoded_prompt, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "generated_tokens = generate_text(model, context, max_new_tokens=800, temperature=0.8, top_k=50, top_p=0.9, repetition_penalty=1.2)\n",
        "print(decode(generated_tokens[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "id": "STdbFUElPHVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2n5b4V5S48q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}