{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "# Read the file into a Python variable\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlr2TWxenx2l",
        "outputId": "54b60454-8ece-489b-b98c-1d529124fb5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-08 05:37:00--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.36M  18.1MB/s    in 0.2s    \n",
            "\n",
            "2025-03-08 05:37:01 (18.1 MB/s) - ‘input.txt’ saved [4573338/4573338]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QVAO98iLkox",
        "outputId": "ae1690fd-ad2a-40d2-e18d-d5203aed1929"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95RMZRJgV8Ui",
        "outputId": "6b6df877-7b99-46de-b343-170f4ab32dca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  4573338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "ngrams = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
        "unique_ngrams = sorted(list(set(ngrams)))\n",
        "vocab_size = len(unique_ngrams)\n",
        "print(\"Vocabulary Size:\", vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J4E1VlIgBqT",
        "outputId": "e936db50-5b57-430f-d43d-99ae45367492"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 15720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ng: i for i, ng in enumerate(unique_ngrams)}\n",
        "itos = {i: ng for i, ng in enumerate(unique_ngrams)}\n",
        "\n",
        "print(\"Sample of stoi:\", dict(list(stoi.items())[:10]))\n",
        "print(\"Sample of itos:\", dict(list(itos.items())[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUNwxdrzWJls",
        "outputId": "cbad8615-8f4d-4942-f1f8-76cf82bb86d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of stoi: {'\\n\\n\\n': 0, '\\n\\nA': 1, '\\n\\nB': 2, '\\n\\nC': 3, '\\n\\nD': 4, '\\n\\nE': 5, '\\n\\nF': 6, '\\n\\nG': 7, '\\n\\nH': 8, '\\n\\nI': 9}\n",
            "Sample of itos: {0: '\\n\\n\\n', 1: '\\n\\nA', 2: '\\n\\nB', 3: '\\n\\nC', 4: '\\n\\nD', 5: '\\n\\nE', 6: '\\n\\nF', 7: '\\n\\nG', 8: '\\n\\nH', 9: '\\n\\nI'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[s[i:i + n]] for i in range(len(s) - n + 1) if s[i:i + n] in stoi]\n",
        "\n",
        "def decode(l):\n",
        "    if not l:\n",
        "        return \"\"\n",
        "    decoded_string = itos[l[0]]\n",
        "    for i in range(1, len(l)):\n",
        "        decoded_string += itos[l[i]][-1]\n",
        "    return decoded_string\n",
        "\n",
        "# Testing the updated functions\n",
        "encoded = encode(\"who am i\")\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef35VXGrWz9H",
        "outputId": "eccd7fde-3257-4816-832f-37c7e40e28d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [15066, 9428, 11783, 1054, 6580, 10896]\n",
            "Decoded: who am i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = int)\n",
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTQdcew8ZwIt",
        "outputId": "17433a9e-738a-43cc-f595-e493168ec031"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3763,  9910, 13248, 13739, 13860,   770,  3262,  9962, 14101, 10004,\n",
              "        15680,  8237, 11323,  2386,   113,  3090,  8112,  8742, 12212, 12979,\n",
              "         7914,  1281, 15036,  7907,  1221, 12648, 13170, 11905,  7218,  8082,\n",
              "         8043,  7490,  1055,  6634, 11694, 15406,  1128,  8792, 14677, 13267,\n",
              "        14061,  9374,  8313, 12847,  1910,  1140,  9359,  8004,  6667, 12820,\n",
              "         1180, 11004,  7910,  1244, 13701, 12544,  7999,  6537, 10179,  2298,\n",
              "            1,    94,  2923, 10633, 10446,  2403,   376,  5693, 12544,  7999,\n",
              "         6535, 10151,  1921,  1244, 13701, 12544,  7999,  6537, 10179,  2298,\n",
              "            6,   187,  3763,  9910, 13248, 13739, 13860,   770,  3262,  9962,\n",
              "        14101, 10004, 15680,  8237, 11323,  2408,   431,  6213, 12280, 14309])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "n_embed = 256          # Size of embeddings\n",
        "n_head = 8             # Number of attention heads\n",
        "n_layer = 6            # Number of transformer layers\n",
        "batch_size = 32        # Batch size\n",
        "block_size = 512       # Context length\n",
        "max_iters = 5000       # Training iterations\n",
        "eval_interval = 500    # Evaluation interval\n",
        "learning_rate = 5e-4   # Learning rate\n",
        "eval_iters = 200       # Evaluation steps for validation\n",
        "dropout = 0.2          # Dropout probability\n",
        "weight_decay = 1e-4    # Weight decay for regularization\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n"
      ],
      "metadata": {
        "id": "z8NEKZqAXEo8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "n = int(0.85 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "5vmHH7umZ52e"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch_random(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    max_index = len(data) - block_size - 1\n",
        "    random_index = torch.randint(max_index, (batch_size,))\n",
        "    x = torch.stack([data[i: i + block_size] for i in random_index])\n",
        "    y = torch.stack([data[(i + 1): (i + 1) + block_size] for i in random_index])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch_random(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "x_batch, y_batch = get_batch_random('train')\n",
        "print('inputs:')\n",
        "print(x_batch.shape)\n",
        "print(x_batch)\n",
        "print('targets:')\n",
        "print(y_batch.shape)\n",
        "print(y_batch)\n"
      ],
      "metadata": {
        "id": "B8UdT3oWagT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29506fd1-83be-44f2-a7e5-894da9bae088"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([32, 512])\n",
            "tensor([[ 1207, 12199, 12827,  ..., 14061,  9370,  8231],\n",
            "        [13508,  6667, 12815,  ..., 13560,  8339, 13295],\n",
            "        [ 1255, 14061,  9350,  ..., 11413,  7892,  1055],\n",
            "        ...,\n",
            "        [14509,  9900, 12979,  ..., 11465,  8992,  8199],\n",
            "        [ 8323, 12981,  7931,  ..., 14686, 13443,  1850],\n",
            "        [ 4208,  4742,  3172,  ..., 11102, 12212, 12979]])\n",
            "targets:\n",
            "torch.Size([32, 512])\n",
            "tensor([[12199, 12827,  1255,  ...,  9370,  8231, 11274],\n",
            "        [ 6667, 12815,  1139,  ...,  8339, 13295, 14791],\n",
            "        [14061,  9350,  7892,  ...,  7892,  1055,  6627],\n",
            "        ...,\n",
            "        [ 9900, 12979,  7900,  ...,  8992,  8199, 10677],\n",
            "        [12981,  7931,  1777,  ..., 13443,  1850,   421],\n",
            "        [ 4742,  3172,  3475,  ..., 12212, 12979,  7897]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using bigram language model\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.self_attention_head = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.self_attention_head(self.ln1(x))\n",
        "        x = x + self.fforward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) #(Batch, Time, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets) # quality of logits based on targets\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = NgramLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "print(next(model.parameters()).device)  # Should print \"cuda:0\" if using GPU\n",
        "\n",
        "\n",
        "logits, loss = model(x_batch, y_batch)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype = torch.long).to(device)\n",
        "print(decode(model.generate(idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrahGtfZeDzc",
        "outputId": "c9743f05-2634-4587-a9c8-21281a3b162d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([16384, 15720])\n",
            "tensor(9.8266, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "\n",
            "\n",
            "rlawglpcRwic\n",
            "faekjbm orHap;eofz:h \n",
            " g?;soi. sm;slctQfur?Rew]PScpfo bOAgl-Lio,Dupsa.ed,l\n",
            "cHo?ure-uikJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "Xrc8GyfxjFXO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for iter in range(max_iters):\n",
        "    if (iter % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    x_batch, y_batch = get_batch_random('train')\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "jCv_k4mqoWfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c01891d-bb8b-48e9-86e6-b7c5673b917b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 9.827683448791504, val loss: 9.8255\n",
            "step: 500, train loss: 1.6809524297714233, val loss: 1.9484\n",
            "step: 1000, train loss: 1.5908868312835693, val loss: 1.8834\n",
            "step: 1500, train loss: 1.4464364051818848, val loss: 1.7728\n",
            "step: 2000, train loss: 1.3373647928237915, val loss: 1.7080\n",
            "step: 2500, train loss: 1.2660856246948242, val loss: 1.6683\n",
            "step: 3000, train loss: 1.2172232866287231, val loss: 1.6565\n",
            "step: 3500, train loss: 1.1778159141540527, val loss: 1.6443\n",
            "step: 4000, train loss: 1.1415619850158691, val loss: 1.6385\n",
            "step: 4500, train loss: 1.112372636795044, val loss: 1.6510\n",
            "1.1112065315246582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'ngram_language_model.pth')\n"
      ],
      "metadata": {
        "id": "vXRaCmmRS4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NgramLanguageModel(vocab_size)  # Initialize model architecture\n",
        "\n",
        "# If using CPU\n",
        "model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# If using GPU\n",
        "# model.load_state_dict(torch.load('ngram_language_model.pth'))  # Load saved parameters\n",
        "#model = model.to(device)  # Move to GPU if available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_XJtJhCVt7t",
        "outputId": "a0ca3db2-dd5a-47fd-f453-fe051a8625be"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-e8c17731f1a0>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
        "print(decode(model.generate(context, max_new_tokens = 400)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "VT9uiB-uVugJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b8db00-ace8-42a9-e59e-8b2c38c44359"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "This is nowness r that French king.\n",
            "\n",
            "VERGES:\n",
            "That I scourge for this business.\n",
            "\n",
            "VERGES:\n",
            "In the soldier: would I would come him,\n",
            "By Cupid's flinty three three. I am sworn.\n",
            "\n",
            "PROSPERO:\n",
            "Thou hast seen, so ignobly go he quarrel?\n",
            "\n",
            "ESCALUS:\n",
            "Anon, but pray the moon shall not have eafteaves:\n",
            "I am all, infect. And what I already thanks\n",
            "Is royal epitaphs, and this is not so:\n",
            "Is it my fellow: for the love to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            context_cond = context[:, -block_size:]\n",
        "            logits, _ = model(context_cond)\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "            top_k_probs /= top_k_probs.sum()  # Normalize\n",
        "            idx_next = torch.multinomial(top_k_probs, num_samples=1)\n",
        "            idx_next = top_k_indices.gather(1, idx_next)\n",
        "\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "    return context\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(generate_text(model, context, max_new_tokens=800, temperature=0.8, top_k=50)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "sBG-p9aAkl9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b6dc52-6514-4131-b46d-18c92e8fa5b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "nown it.\n",
            "\n",
            "BIRON:\n",
            "Yea, I can you say the Lord for your come to make him every one hither\n",
            "sworn\n",
            "you to go and make any other thing?\n",
            "\n",
            "HOLOFERNES:\n",
            "Take you this a when reasons to him. I have a\n",
            "Rosalind your answer.\n",
            "\n",
            "BASSANIO:\n",
            "In hanged my lord cardinal; in the ale\n",
            "of name of good oath.\n",
            "\n",
            "BASSANIO:\n",
            "Shylocks it is that he he is in truth?\n",
            "\n",
            "HOLOFERNES:\n",
            "Most what will fetch me with upon yonder.\n",
            "\n",
            "BOYET:\n",
            "And I am concludes this true; but in\n",
            "me, if it will some discovered and the readies.\n",
            "\n",
            "FENTON:\n",
            "It is both at France; for my rememediculty, as 'scaps soon as\n",
            "lishes it as again! Were as mine an inste, the\n",
            "sing ass-grment to cudgel us from the rescues of my\n",
            "guation: but he thence and not diet\n",
            "\n",
            "ESCALUS:\n",
            "We have you fair to do it.\n",
            "\n",
            "BALTHASAR:\n",
            "Being could porter than not.\n",
            "\n",
            "FERDINAND:\n",
            "Nay, sir, for he would \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.2):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Safeguard for block size\n",
        "            context_cond = context[:, -min(block_size, context.size(1)):]\n",
        "\n",
        "            # Check if context is empty\n",
        "            if context_cond.size(1) == 0:\n",
        "                raise ValueError(\"Context is empty!\")\n",
        "\n",
        "            logits, _ = model(context_cond)\n",
        "\n",
        "            # Check if logits are empty\n",
        "            if logits.size(1) == 0:\n",
        "                raise ValueError(\"Logits are empty!\")\n",
        "\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "\n",
        "            # Repetition penalty\n",
        "            for token_id in set(context.view(-1).tolist()):\n",
        "                logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k > 0:\n",
        "                top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "                probs = torch.zeros_like(probs).scatter(1, top_k_indices, top_k_probs)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "            for i in range(probs.size(0)):\n",
        "                probs[i, sorted_indices[i][sorted_indices_to_remove[i]]] = 0\n",
        "\n",
        "            probs /= probs.sum()  # Normalize after filtering\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "\n",
        "            # Early stopping if the model keeps repeating\n",
        "            if idx_next.item() == context[:, -2].item():\n",
        "                break\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"You are all resolved rather to die than to famish?\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor(encoded_prompt, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "generated_tokens = generate_text(model, context, max_new_tokens=800, temperature=0.8, top_k=50, top_p=0.9, repetition_penalty=1.2)\n",
        "print(decode(generated_tokens[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezpn0uHdUT5",
        "outputId": "84260461-c219-4ed1-a44e-d205621f639e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "FALSTAFF:\n",
            "Mistress Ford's service of it.\n",
            "\n",
            "PRINCE HENRY:\n",
            "Why shall, sir, what we mend this hour is, thy\n",
            "king on and\n",
            "three for fear: he will be hardly out of your\n",
            "conscience. I am at\n",
            "wifest till a fire, or, as I humbly tender.\n",
            "The knave, good man, yet has much by an\n",
            "heavy counteries those unclay from him old.\n",
            "Go in carry it after above God's chamber,\n",
            "when he's death.\n",
            "\n",
            "POINS:\n",
            "I pray, I come not from you, if I\n",
            "may; it blastink thee bravely.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "How my mind, most, howlines,\n",
            "smaches: O, let me speak off him, in it\n",
            "his worth had wash'd against\n",
            "To try, better his business,--as, fully-rook, thus:\n",
            "Being near something: advised!\n",
            "\n",
            "SHYLOCK:\n",
            "'Twas garmed, Othello,\n",
            "Some little cup inch, earney,\n",
            "That, flier: take your mouth,\n",
            "As imperfection.\n",
            "Conduct you: stand, certain; and both\n",
            "talk away: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SELF ATTENTION***"
      ],
      "metadata": {
        "id": "DzMtfLtPbf-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "x_bag_of_words = torch.zeros((B, T, C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        x_prev = x[b, :t + 1] # t, C\n",
        "        x_bag_of_words[b, t] = torch.mean(x_prev, 0)"
      ],
      "metadata": {
        "id": "PnI3jGKNWaTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2 (optimize above one)\n",
        "wei = torch.tril(torch.ones(T, T)) # crete bottom triangle matrix(can average the ones which are 1, else not)\n",
        "wei = wei / wei.sum(1, keepdim = True)\n",
        "x_bag_of_words2 = wei @ x # (T, T) @ (B, T, C) --> (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "x_bag_of_words2"
      ],
      "metadata": {
        "id": "rkF59G_Vh-12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(x_bag_of_words, x_bag_of_words2) # if True -> both same"
      ],
      "metadata": {
        "id": "QSNof9F-iTGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3 (Softmax)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))"
      ],
      "metadata": {
        "id": "zdZXxv7TiUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SELF ATTENTION\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Single head of self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "#out = wei @ x\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape\n"
      ],
      "metadata": {
        "id": "dXFtm0iaaEHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CODE**"
      ],
      "metadata": {
        "id": "G9-fDUINMztP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Download the Shakespeare dataset\n",
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "# Read the file into a Python variable\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "# Download the Shakespeare dataset\n",
        "n = 3\n",
        "ngrams = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
        "unique_ngrams = sorted(list(set(ngrams)))\n",
        "vocab_size = len(unique_ngrams)\n",
        "\n",
        "\n",
        "# Create mappings from n-grams to indices and vice versa\n",
        "stoi = {ng: i for i, ng in enumerate(unique_ngrams)}\n",
        "itos = {i: ng for i, ng in enumerate(unique_ngrams)}\n",
        "\n",
        "\n",
        "# Encode function: Convert string to a list of indices\n",
        "encode = lambda s: [stoi[s[i:i + n]] for i in range(len(s) - n + 1) if s[i:i + n] in stoi]\n",
        "\n",
        "\n",
        "# Decode function: Convert list of indices back to string\n",
        "def decode(l):\n",
        "    if not l:\n",
        "        return \"\"\n",
        "    decoded_string = itos[l[0]]\n",
        "    for i in range(1, len(l)):\n",
        "        decoded_string += itos[l[i]][-1]\n",
        "    return decoded_string\n",
        "\n",
        "\n",
        "# Encode the text into indices and convert to a tensor\n",
        "data = torch.tensor(encode(text), dtype = int)\n",
        "\n",
        "\n",
        "# Model hyperparameters\n",
        "n_embed = 256          # Size of embeddings\n",
        "n_head = 8             # Number of attention heads\n",
        "n_layer = 6            # Number of transformer layers\n",
        "batch_size = 32        # Batch size\n",
        "block_size = 512       # Context length\n",
        "max_iters = 5000       # Training iterations\n",
        "eval_interval = 500    # Evaluation interval\n",
        "learning_rate = 5e-4   # Learning rate\n",
        "eval_iters = 200       # Evaluation steps for validation\n",
        "dropout = 0.2          # Dropout probability\n",
        "weight_decay = 1e-4    # Weight decay for regularization\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
        "\n",
        "\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "n = int(0.85 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "# Fix seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Function to get a random batch of data\n",
        "def get_batch_random(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    max_index = len(data) - block_size - 1\n",
        "    random_index = torch.randint(max_index, (batch_size,))\n",
        "    x = torch.stack([data[i: i + block_size] for i in random_index])\n",
        "    y = torch.stack([data[(i + 1): (i + 1) + block_size] for i in random_index])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Function to estimate loss without updating model\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch_random(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Define Transformer-based language model components\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Self-attention head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Self-attention head\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Feed-forward network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Transformer block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.self_attention_head = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.self_attention_head(self.ln1(x))\n",
        "        x = x + self.fforward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Language model class\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(T, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) #(Batch, Time, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets) # quality of logits based on targets\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "model = NgramLanguageModel(vocab_size)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    if (iter % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    x_batch, y_batch = get_batch_random('train')\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'ngram_language_model.pth')\n",
        "\n",
        "# Load model for inference\n",
        "model = NgramLanguageModel(vocab_size)  # Initialize model architecture\n",
        "\n",
        "# If using CPU\n",
        "model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# If using GPU\n",
        "# model.load_state_dict(torch.load('ngram_language_model.pth'))  # Load saved parameters\n",
        "#model = model.to(device)  # Move to GPU if available\n",
        "\n",
        "\n",
        "# Generate text based on a prompt\n",
        "def generate_text(model, context, max_new_tokens, temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.2):\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Safeguard for block size\n",
        "            context_cond = context[:, -min(block_size, context.size(1)):]\n",
        "\n",
        "            # Check if context is empty\n",
        "            if context_cond.size(1) == 0:\n",
        "                raise ValueError(\"Context is empty!\")\n",
        "\n",
        "            logits, _ = model(context_cond)\n",
        "\n",
        "            # Check if logits are empty\n",
        "            if logits.size(1) == 0:\n",
        "                raise ValueError(\"Logits are empty!\")\n",
        "\n",
        "            logits = logits[:, -1, :] / temperature  # Adjust randomness\n",
        "\n",
        "            # Repetition penalty\n",
        "            for token_id in set(context.view(-1).tolist()):\n",
        "                logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k > 0:\n",
        "                top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
        "                probs = torch.zeros_like(probs).scatter(1, top_k_indices, top_k_probs)\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "            for i in range(probs.size(0)):\n",
        "                probs[i, sorted_indices[i][sorted_indices_to_remove[i]]] = 0\n",
        "\n",
        "            probs /= probs.sum()  # Normalize after filtering\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            context = torch.cat((context, idx_next), dim=1)\n",
        "\n",
        "            # Early stopping if the model keeps repeating\n",
        "            if idx_next.item() == context[:, -2].item():\n",
        "                break\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "\n",
        "prompt = \"You are all resolved rather to die than to famish?\"\n",
        "encoded_prompt = encode(prompt)\n",
        "context = torch.tensor(encoded_prompt, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "generated_tokens = generate_text(model, context, max_new_tokens=800, temperature=0.8, top_k=50, top_p=0.9, repetition_penalty=1.2)\n",
        "print(decode(generated_tokens[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "id": "STdbFUElPHVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2n5b4V5S48q"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}